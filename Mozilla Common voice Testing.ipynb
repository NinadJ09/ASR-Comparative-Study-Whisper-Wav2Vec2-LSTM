{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec5b4a5-f180-46c4-b3b2-95ff3d5be396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, tarfile, pathlib\n",
    "\n",
    "ARCHIVE = \"cv-corpus-11.0-2022-09-21-en.tar.gz\"\n",
    "OUT_DIR = \"data/cv11_en\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "want = [\n",
    "    \"cv-corpus-11.0-2022-09-21/en/validated.tsv\",\n",
    "    \"cv-corpus-11.0-2022-09-21/en/dev.tsv\",\n",
    "    \"cv-corpus-11.0-2022-09-21/en/test.tsv\",\n",
    "    \"cv-corpus-11.0-2022-09-21/en/train.tsv\",\n",
    "    \"cv-corpus-11.0-2022-09-21/en/clips\"\n",
    "]\n",
    "\n",
    "with tarfile.open(ARCHIVE, \"r:gz\") as tf:\n",
    "    for m in tf.getmembers():\n",
    "        if any(m.name.startswith(w) for w in want):\n",
    "            tf.extract(m, OUT_DIR)\n",
    "\n",
    "print(\"Done. Extracted into:\", pathlib.Path(OUT_DIR).resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826deade-daad-4fc9-b614-51762d6bd1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_DIR = \"/Users/ninadjoshi/asr-comparative-study/ASR-Comparative-Study-Thesis/asr-comparative-study/notebooks/data/cv11_en\"\n",
    "print(os.listdir(DATA_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30bd9777-307b-4e26-b569-c9281553f095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ninadjoshi/asr-comparative-study/ASR-Comparative-Study-Thesis/asr-comparative-study/notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cebd9fb7-de16-49dc-9170-6f661e666df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV_ROOT: /Users/ninadjoshi/asr-comparative-study/ASR-Comparative-Study-Thesis/asr-comparative-study/notebooks/data/cv11_en/cv-corpus-11.0-2022-09-21/en\n",
      "Exists? True\n",
      "Files: [PosixPath('/Users/ninadjoshi/asr-comparative-study/ASR-Comparative-Study-Thesis/asr-comparative-study/notebooks/data/cv11_en/cv-corpus-11.0-2022-09-21/en/train.tsv'), PosixPath('/Users/ninadjoshi/asr-comparative-study/ASR-Comparative-Study-Thesis/asr-comparative-study/notebooks/data/cv11_en/cv-corpus-11.0-2022-09-21/en/dev.tsv'), PosixPath('/Users/ninadjoshi/asr-comparative-study/ASR-Comparative-Study-Thesis/asr-comparative-study/notebooks/data/cv11_en/cv-corpus-11.0-2022-09-21/en/validated.tsv'), PosixPath('/Users/ninadjoshi/asr-comparative-study/ASR-Comparative-Study-Thesis/asr-comparative-study/notebooks/data/cv11_en/cv-corpus-11.0-2022-09-21/en/test.tsv')]\n",
      "                           path  \\\n",
      "0  common_voice_en_27710027.mp3   \n",
      "1    common_voice_en_699711.mp3   \n",
      "2  common_voice_en_21953345.mp3   \n",
      "\n",
      "                                            sentence  \\\n",
      "0  Joe Keaton disapproved of films, and Buster al...   \n",
      "1                               She'll be all right.   \n",
      "2                                                six   \n",
      "\n",
      "                                          audio_path  \n",
      "0  /Users/ninadjoshi/asr-comparative-study/ASR-Co...  \n",
      "1  /Users/ninadjoshi/asr-comparative-study/ASR-Co...  \n",
      "2  /Users/ninadjoshi/asr-comparative-study/ASR-Co...  \n",
      "Clip exists? True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os, pandas as pd\n",
    "\n",
    "# 1) Pointing  to the folder where you extracted the archive\n",
    "DATA_DIR = Path(\"/Users/ninadjoshi/asr-comparative-study/ASR-Comparative-Study-Thesis/asr-comparative-study/notebooks/data/cv11_en\")\n",
    "\n",
    "# 2) Build the Common Voice root (version + language)\n",
    "CV_VERSION = \"cv-corpus-11.0-2022-09-21\"\n",
    "LANG = \"en\"\n",
    "CV_ROOT = DATA_DIR / CV_VERSION / LANG\n",
    "CLIPS_DIR = CV_ROOT / \"clips\"\n",
    "\n",
    "print(\"CV_ROOT:\", CV_ROOT)\n",
    "print(\"Exists?\", CV_ROOT.exists())\n",
    "print(\"Files:\", list(CV_ROOT.glob(\"*.tsv\"))[:5])\n",
    "\n",
    "# 3) Reading the splits (TSV, tab-delimited)\n",
    "meta = pd.read_csv(CV_ROOT / \"validated.tsv\", sep=\"\\t\")\n",
    "# or choose 'dev.tsv' / 'test.tsv' / 'train.tsv'\n",
    "\n",
    "# 4) Adding absolute audio paths\n",
    "meta[\"audio_path\"] = meta[\"path\"].apply(lambda p: str(CLIPS_DIR / p))\n",
    "\n",
    "# quick sanity check\n",
    "print(meta.head(3)[[\"path\", \"sentence\", \"audio_path\"]])\n",
    "print(\"Clip exists?\", os.path.exists(meta.loc[0, \"audio_path\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "460514b7-24a5-40b1-b8a2-de8b8d0a68b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "small = meta.sample(200, random_state=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc8fa96a-20f6-439b-b309-a213fce51972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "def load_batch(batch):\n",
    "    audio, sr = sf.read(batch[\"audio_path\"])\n",
    "    return {\n",
    "        \"audio\": audio,\n",
    "        \"sampling_rate\": sr,\n",
    "        \"sentence\": batch[\"sentence\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "345a56fb-d470-4c5c-849b-e657a6a0fca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing a manageable sample for now (e.g. 5k examples)\n",
    "sample = meta.sample(5000, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "845145bc-81d8-4e42-961c-8af8d41ded7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "def load_batch(batch):\n",
    "    audio, sr = sf.read(batch[\"audio_path\"])\n",
    "    return {\n",
    "        \"audio\": audio,\n",
    "        \"sampling_rate\": sr,\n",
    "        \"sentence\": batch[\"sentence\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86876838-0dea-4eb3-a7eb-43f9bdd6dfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa \n",
    "\n",
    "def load_audio(path):\n",
    "    \"\"\"Read audio as float32 in [-1, 1] and return (wave, sr).\"\"\"\n",
    "    audio, sr = sf.read(path, always_2d=False)\n",
    "    # Ensure float32\n",
    "    if audio.dtype != np.float32:\n",
    "        audio = audio.astype(np.float32, copy=False)\n",
    "    return audio, sr\n",
    "\n",
    "def to_mono(wave):\n",
    "    \"\"\"Collapse multi-channel to mono.\"\"\"\n",
    "    if wave.ndim == 1:\n",
    "        return wave\n",
    "    # soundfile returns shape (n_samples, n_channels) sometimes; handle both\n",
    "    if wave.shape[0] < wave.shape[-1]:  # (channels, samples)\n",
    "        wave = wave.T\n",
    "    return librosa.to_mono(wave.T).astype(np.float32)\n",
    "\n",
    "def resample(wave, sr, target_sr):\n",
    "    \"\"\"Resample to target_sr using librosa.\"\"\"\n",
    "    if sr == target_sr:\n",
    "        return wave\n",
    "    wave = to_mono(wave)\n",
    "    return librosa.resample(wave, orig_sr=sr, target_sr=target_sr).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63218653-06a8-4ab5-85e4-3285e784e29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "WHISPER_ID = \"openai/whisper-small\"  # or \"medium\", \"base\", etc.\n",
    "wh_proc  = WhisperProcessor.from_pretrained(WHISPER_ID)\n",
    "wh_model = WhisperForConditionalGeneration.from_pretrained(WHISPER_ID).to(device).eval()\n",
    "WHISPER_SR = wh_proc.feature_extractor.sampling_rate  # typically 16000\n",
    "\n",
    "GEN_KW = dict(task=\"transcribe\", language=\"en\", num_beams=3, length_penalty=1.0,\n",
    "              no_repeat_ngram_size=3, temperature=0.0)\n",
    "\n",
    "def transcribe_whisper_path(path):\n",
    "    wave, sr = load_audio(path)\n",
    "    wave16 = resample(wave, sr, WHISPER_SR)\n",
    "    inputs = wh_proc(wave16, sampling_rate=WHISPER_SR, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        ids = wh_model.generate(inputs.input_features, **GEN_KW)\n",
    "    text = wh_proc.batch_decode(ids, skip_special_tokens=True)[0]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93a9dfe0-254a-41a2-8597-b3df3106c7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "\n",
    "W2V_ID   = \"facebook/wav2vec2-large-960h-lv60-self\"\n",
    "w2v_proc = Wav2Vec2Processor.from_pretrained(W2V_ID)\n",
    "w2v_model= Wav2Vec2ForCTC.from_pretrained(W2V_ID).to(device).eval()\n",
    "W2V_SR   = w2v_proc.feature_extractor.sampling_rate  # typically 16000\n",
    "\n",
    "def transcribe_w2v_path(path):\n",
    "    wave, sr = load_audio(path)\n",
    "    wave16 = resample(wave, sr, W2V_SR)\n",
    "    inputs = w2v_proc(wave16, sampling_rate=W2V_SR, return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits   = w2v_model(**inputs).logits\n",
    "        pred_ids = torch.argmax(logits, dim=-1)\n",
    "    text = w2v_proc.batch_decode(pred_ids, skip_special_tokens=True)[0]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc23f264-cc5e-4c87-a0ae-636f675450cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from jiwer import wer, cer\n",
    "\n",
    "#references = sample[\"sentence\"].tolist()\n",
    "\n",
    "#hyp_whisper = []\n",
    "#hyp_w2v     = []\n",
    "\n",
    "#for _, row in sample.iterrows():\n",
    "    #path = row[\"audio_path\"]\n",
    "    #hyp_whisper.append(transcribe_whisper_path(path))\n",
    "    #hyp_w2v.append(transcribe_w2v_path(path))\n",
    "\n",
    "#print(f\"Whisper  WER: {wer(references, hyp_whisper)*100:.2f}% | CER: {cer(references, hyp_whisper)*100:.2f}%\")\n",
    "#print(f\"Wav2Vec2 WER: {wer(references, hyp_w2v)*100:.2f}% | CER: {cer(references, hyp_w2v)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9061bd7d-8a22-482e-9aab-852ec2031fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: transformers==4.*\n"
     ]
    }
   ],
   "source": [
    "!pip -q install soundfile jiwer transformers==4.* librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d73ce0ac-0110-471c-a3c1-97192dbf49b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os, time, json, math, numpy as np, pandas as pd\n",
    "import torch, soundfile as sf, librosa\n",
    "from jiwer import wer, cer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Fast duration without loading whole file into memory\n",
    "def get_duration_sec(path: str) -> float:\n",
    "    with sf.SoundFile(path) as f:\n",
    "        return len(f) / f.samplerate\n",
    "\n",
    "# Load + resample mono audio to target SR (float32)\n",
    "def load_mono_resampled(path: str, target_sr: int) -> np.ndarray:\n",
    "    wav, sr = sf.read(path, always_2d=False)        # np.float32/64\n",
    "    if wav.ndim > 1:                                # mixdown\n",
    "        wav = wav.mean(axis=1)\n",
    "    if sr != target_sr:\n",
    "        wav = librosa.resample(wav, orig_sr=sr, target_sr=target_sr)\n",
    "    return wav.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2c34bca-f8e1-48c6-9f39-318342ee1079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset: 50 clips | First file: /Users/ninadjoshi/asr-comparative-study/ASR-Comparative-Study-Thesis/asr-comparative-study/notebooks/data/cv11_en/cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_69651.mp3\n"
     ]
    }
   ],
   "source": [
    "N = 50  # start small; scale up later\n",
    "sample_small = sample.sample(N, random_state=0).reset_index(drop=True)\n",
    "\n",
    "refs  = sample_small[\"sentence\"].tolist()\n",
    "paths = sample_small[\"audio_path\"].tolist()\n",
    "print(f\"Subset: {len(paths)} clips | First file: {paths[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72738c70-726f-4812-a9cd-e3769393845f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "\n",
    "# --- Whisper (use 'small' for speed or 'medium' for quality) ---\n",
    "WHISPER_ID = \"openai/whisper-medium\"   # change to \"openai/whisper-small\" if needed\n",
    "wh_asr = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=WHISPER_ID,\n",
    "    device=0 if torch.cuda.is_available() else -1,        # pipeline handles CPU/GPU\n",
    "    generate_kwargs=dict(task=\"transcribe\", language=\"en\", num_beams=5, temperature=0.0)\n",
    ")\n",
    "\n",
    "# --- Wav2Vec2 (16 kHz) ---\n",
    "W2V_ID  = \"facebook/wav2vec2-large-960h-lv60-self\"\n",
    "W2V_SR  = 16000\n",
    "w2v_proc  = Wav2Vec2Processor.from_pretrained(W2V_ID)\n",
    "w2v_model = Wav2Vec2ForCTC.from_pretrained(W2V_ID).to(device).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d17397f-8494-4611-8957-6b607574732a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whisper via pipeline (batch)\n",
    "def transcribe_whisper_paths(paths, batch_size=8):\n",
    "    outs = wh_asr(paths, batch_size=batch_size)\n",
    "    # pipeline returns list[{\"text\": \"...\"}]\n",
    "    return [o[\"text\"].lower().strip() for o in outs]\n",
    "\n",
    "# Wav2Vec2 batched\n",
    "def transcribe_w2v_batch(paths, batch_size=16, target_sr=W2V_SR):\n",
    "    hyps = []\n",
    "    for i in range(0, len(paths), batch_size):\n",
    "        batch_paths = paths[i:i+batch_size]\n",
    "        wavs = [load_mono_resampled(p, target_sr) for p in batch_paths]\n",
    "        inputs = w2v_proc(\n",
    "            wavs, sampling_rate=target_sr, return_tensors=\"pt\", padding=True\n",
    "        ).to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = w2v_model(**inputs).logits\n",
    "            pred_ids = torch.argmax(logits, dim=-1)\n",
    "        texts = w2v_proc.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "        hyps.extend([t.lower().strip() for t in texts])\n",
    "    return hyps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbbab292-bf3f-499e-a09f-4e953baf3315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`return_token_timestamps` is deprecated for WhisperFeatureExtractor and will be removed in Transformers v5. Use `return_attention_mask` instead, as the number of frames can be inferred from it.\n",
      "python(66922) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(66923) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Whisper]  WER: 23.23%  CER: 5.72%  | wall: 5688.9s  RTF: 23.171\n",
      "[Wav2Vec2] WER: 39.14%  CER: 9.63% | wall: 32.5s RTF: 0.132\n"
     ]
    }
   ],
   "source": [
    "# Total audio seconds (for RTF)\n",
    "audio_seconds_total = sum(get_duration_sec(p) for p in paths)\n",
    "\n",
    "# --- Whisper ---\n",
    "t0 = time.time()\n",
    "hyp_whisper = transcribe_whisper_paths(paths, batch_size=8)\n",
    "wh_wall = time.time() - t0\n",
    "wh_rtf  = wh_wall / audio_seconds_total\n",
    "\n",
    "# --- Wav2Vec2 ---\n",
    "t0 = time.time()\n",
    "hyp_w2v = transcribe_w2v_batch(paths, batch_size=16)\n",
    "w2v_wall = time.time() - t0\n",
    "w2v_rtf  = w2v_wall / audio_seconds_total\n",
    "\n",
    "# Metrics\n",
    "wh_wer  = wer(refs, hyp_whisper) * 100\n",
    "wh_cer  = cer(refs, hyp_whisper) * 100\n",
    "w2v_wer = wer(refs, hyp_w2v) * 100\n",
    "w2v_cer = cer(refs, hyp_w2v) * 100\n",
    "\n",
    "print(f\"[Whisper]  WER: {wh_wer:.2f}%  CER: {wh_cer:.2f}%  | wall: {wh_wall:.1f}s  RTF: {wh_rtf:.3f}\")\n",
    "print(f\"[Wav2Vec2] WER: {w2v_wer:.2f}%  CER: {w2v_cer:.2f}% | wall: {w2v_wall:.1f}s RTF: {w2v_rtf:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b910223b-f894-4b5c-975f-7763a4af2764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      "- runs/cv11_subset_50_20250905-084801.csv\n",
      "- runs/cv11_subset_50_20250905-084801.json\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"runs\", exist_ok=True)\n",
    "\n",
    "df_out = pd.DataFrame({\n",
    "    \"path\": paths,\n",
    "    \"ref\": refs,\n",
    "    \"whisper\": hyp_whisper,\n",
    "    \"wav2vec2\": hyp_w2v,\n",
    "})\n",
    "ts = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "csv_path  = f\"runs/cv11_subset_{len(paths)}_{ts}.csv\"\n",
    "json_path = f\"runs/cv11_subset_{len(paths)}_{ts}.json\"\n",
    "\n",
    "df_out.to_csv(csv_path, index=False)\n",
    "\n",
    "run = dict(\n",
    "    dataset=\"Common Voice 11 (subset)\",\n",
    "    n_items=len(paths),\n",
    "    audio_s=audio_seconds_total,\n",
    "    whisper=dict(model=WHISPER_ID, WER=wh_wer, CER=wh_cer, wall_s=wh_wall, RTF=wh_rtf),\n",
    "    wav2vec2=dict(model=W2V_ID, WER=w2v_wer, CER=w2v_cer, wall_s=w2v_wall, RTF=w2v_rtf),\n",
    ")\n",
    "\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(run, f, indent=2)\n",
    "\n",
    "print(f\"Saved:\\n- {csv_path}\\n- {json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbd3510-3e2f-4ba5-957b-55ed02285d14",
   "metadata": {},
   "source": [
    "## Graphical presentation of the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9b7594f-c450-4744-9081-b9bf61a00e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>WER</th>\n",
       "      <th>CER</th>\n",
       "      <th>wall_s</th>\n",
       "      <th>RTF</th>\n",
       "      <th>items</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Whisper</td>\n",
       "      <td>23.225806</td>\n",
       "      <td>5.724907</td>\n",
       "      <td>5688.880412</td>\n",
       "      <td>23.170930</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wav2Vec2</td>\n",
       "      <td>39.139785</td>\n",
       "      <td>9.628253</td>\n",
       "      <td>32.520292</td>\n",
       "      <td>0.132456</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model        WER       CER       wall_s        RTF  items\n",
       "0   Whisper  23.225806  5.724907  5688.880412  23.170930     50\n",
       "1  Wav2Vec2  39.139785  9.628253    32.520292   0.132456     50"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>ref</th>\n",
       "      <th>hyp_whisper</th>\n",
       "      <th>hyp_w2v</th>\n",
       "      <th>WER_w</th>\n",
       "      <th>CER_w</th>\n",
       "      <th>MER_w</th>\n",
       "      <th>WIL_w</th>\n",
       "      <th>WIP_w</th>\n",
       "      <th>WER_v</th>\n",
       "      <th>CER_v</th>\n",
       "      <th>MER_v</th>\n",
       "      <th>WIL_v</th>\n",
       "      <th>WIP_v</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>And what's the matter with a thousand dollars?</td>\n",
       "      <td>and what's the matter with a thousand dollars?</td>\n",
       "      <td>and what's the mater with a thousand dolars</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>2.173913</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>23.437500</td>\n",
       "      <td>76.562500</td>\n",
       "      <td>37.500000</td>\n",
       "      <td>8.695652</td>\n",
       "      <td>37.500000</td>\n",
       "      <td>60.937500</td>\n",
       "      <td>39.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Bedoya's line has become widely imitated by en...</td>\n",
       "      <td>bidoya's line has become widely imitated by en...</td>\n",
       "      <td>bidoe's line has become widely imitated by ent...</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>3.508772</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>23.437500</td>\n",
       "      <td>76.562500</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>8.771930</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>43.750000</td>\n",
       "      <td>56.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>An hour later, he had before him a chest of Sp...</td>\n",
       "      <td>an hour later he had before him a chest of spa...</td>\n",
       "      <td>an hour later he had before him a chest of spa...</td>\n",
       "      <td>23.076923</td>\n",
       "      <td>4.761905</td>\n",
       "      <td>23.076923</td>\n",
       "      <td>40.828402</td>\n",
       "      <td>59.171598</td>\n",
       "      <td>30.769231</td>\n",
       "      <td>6.349206</td>\n",
       "      <td>30.769231</td>\n",
       "      <td>52.071006</td>\n",
       "      <td>47.928994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Ladies and gentlemen, this is your captain spe...</td>\n",
       "      <td>ladies and gentlemen, this is your captain spe...</td>\n",
       "      <td>ladies and gentlemen this is your captain spea...</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>1.923077</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>23.437500</td>\n",
       "      <td>76.562500</td>\n",
       "      <td>37.500000</td>\n",
       "      <td>5.769231</td>\n",
       "      <td>37.500000</td>\n",
       "      <td>60.937500</td>\n",
       "      <td>39.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Alastair McDonald also sang a song about trams...</td>\n",
       "      <td>alastair macdonald also sang a song about tram...</td>\n",
       "      <td>elister macdonald also sang a song about trams...</td>\n",
       "      <td>30.769231</td>\n",
       "      <td>17.105263</td>\n",
       "      <td>28.571429</td>\n",
       "      <td>45.054945</td>\n",
       "      <td>54.945055</td>\n",
       "      <td>30.769231</td>\n",
       "      <td>21.052632</td>\n",
       "      <td>28.571429</td>\n",
       "      <td>45.054945</td>\n",
       "      <td>54.945055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   i                                                ref  \\\n",
       "0  0     And what's the matter with a thousand dollars?   \n",
       "1  1  Bedoya's line has become widely imitated by en...   \n",
       "2  2  An hour later, he had before him a chest of Sp...   \n",
       "3  3  Ladies and gentlemen, this is your captain spe...   \n",
       "4  4  Alastair McDonald also sang a song about trams...   \n",
       "\n",
       "                                         hyp_whisper  \\\n",
       "0     and what's the matter with a thousand dollars?   \n",
       "1  bidoya's line has become widely imitated by en...   \n",
       "2  an hour later he had before him a chest of spa...   \n",
       "3  ladies and gentlemen, this is your captain spe...   \n",
       "4  alastair macdonald also sang a song about tram...   \n",
       "\n",
       "                                             hyp_w2v      WER_w      CER_w  \\\n",
       "0        and what's the mater with a thousand dolars  12.500000   2.173913   \n",
       "1  bidoe's line has become widely imitated by ent...  12.500000   3.508772   \n",
       "2  an hour later he had before him a chest of spa...  23.076923   4.761905   \n",
       "3  ladies and gentlemen this is your captain spea...  12.500000   1.923077   \n",
       "4  elister macdonald also sang a song about trams...  30.769231  17.105263   \n",
       "\n",
       "       MER_w      WIL_w      WIP_w      WER_v      CER_v      MER_v  \\\n",
       "0  12.500000  23.437500  76.562500  37.500000   8.695652  37.500000   \n",
       "1  12.500000  23.437500  76.562500  25.000000   8.771930  25.000000   \n",
       "2  23.076923  40.828402  59.171598  30.769231   6.349206  30.769231   \n",
       "3  12.500000  23.437500  76.562500  37.500000   5.769231  37.500000   \n",
       "4  28.571429  45.054945  54.945055  30.769231  21.052632  28.571429   \n",
       "\n",
       "       WIL_v      WIP_v  \n",
       "0  60.937500  39.062500  \n",
       "1  43.750000  56.250000  \n",
       "2  52.071006  47.928994  \n",
       "3  60.937500  39.062500  \n",
       "4  45.054945  54.945055  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper: wall=5688.9s  RTF=23.171 | Wav2Vec2: wall=32.5s  RTF=0.132\n",
      "Saved: runs/model_summary.csv and runs/utterance_measures.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Imports & a small compatibility shim ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from jiwer import wer, cer, mer, wil, wip   # public functions\n",
    "\n",
    "def compute_measures_compat(truth, hyp):\n",
    "    \"\"\"\n",
    "    Replacement for jiwer.compute_measures.\n",
    "    Returns the same keys + CER for convenience.\n",
    "    Values are FRACTIONS (0..1) to stay consistent with jiwer.* functions.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"wer\": wer(truth, hyp),\n",
    "        \"cer\": cer(truth, hyp),\n",
    "        \"mer\": mer(truth, hyp),\n",
    "        \"wil\": wil(truth, hyp),\n",
    "        \"wip\": wip(truth, hyp),\n",
    "    }\n",
    "\n",
    "# ---------- Model summary (one row per model) ----------\n",
    "# Using the variable names.\n",
    "summary = []\n",
    "\n",
    "summary.append({\n",
    "    \"model\": \"Whisper\",\n",
    "    \"WER\": wer(refs, hyp_whisper)*100,\n",
    "    \"CER\": cer(refs, hyp_whisper)*100,\n",
    "    \"wall_s\": wh_wall,\n",
    "    \"RTF\": wh_rtf,\n",
    "    \"items\": len(refs),\n",
    "})\n",
    "\n",
    "summary.append({\n",
    "    \"model\": \"Wav2Vec2\",\n",
    "    \"WER\": wer(refs, hyp_w2v)*100,\n",
    "    \"CER\": cer(refs, hyp_w2v)*100,\n",
    "    \"wall_s\": w2v_wall,\n",
    "    \"RTF\": w2v_rtf,\n",
    "    \"items\": len(refs),\n",
    "})\n",
    "\n",
    "df_models = pd.DataFrame(summary)\n",
    "display(df_models)\n",
    "\n",
    "# ---------- Per-utterance table ----------\n",
    "rows = []\n",
    "for i, (r, h_w, h_v) in enumerate(zip(refs, hyp_whisper, hyp_w2v)):\n",
    "    m_w = compute_measures_compat(r, h_w)\n",
    "    m_v = compute_measures_compat(r, h_v)\n",
    "    rows.append({\n",
    "        \"i\": i,\n",
    "        \"ref\": r,\n",
    "        \"hyp_whisper\": h_w,\n",
    "        \"hyp_w2v\": h_v,\n",
    "        \"WER_w\": m_w[\"wer\"]*100,\n",
    "        \"CER_w\": m_w[\"cer\"]*100,\n",
    "        \"MER_w\": m_w[\"mer\"]*100,\n",
    "        \"WIL_w\": m_w[\"wil\"]*100,\n",
    "        \"WIP_w\": m_w[\"wip\"]*100,\n",
    "        \"WER_v\": m_v[\"wer\"]*100,\n",
    "        \"CER_v\": m_v[\"cer\"]*100,\n",
    "        \"MER_v\": m_v[\"mer\"]*100,\n",
    "        \"WIL_v\": m_v[\"wil\"]*100,\n",
    "        \"WIP_v\": m_v[\"wip\"]*100,\n",
    "    })\n",
    "\n",
    "df_utts = pd.DataFrame(rows)\n",
    "\n",
    "# Example previously built an array \"durations_s\" aligned with refs:\n",
    "# df_utts[\"duration_s\"] = durations_s\n",
    "\n",
    "display(df_utts.head())\n",
    "\n",
    "# ---------- Plots ----------\n",
    "# 1) Bar chart: corpus-level WER & CER\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "x = np.arange(len(df_models))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, df_models[\"WER\"], width, label=\"WER\")\n",
    "ax.bar(x + width/2, df_models[\"CER\"], width, label=\"CER\")\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df_models[\"model\"])\n",
    "ax.set_ylabel(\"Error rate (%)\")\n",
    "ax.set_title(\"Corpus-level WER/CER\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "# 2) Histograms: utternace-level WER distributions\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.hist(df_utts[\"WER_w\"].dropna(), bins=20, alpha=0.6, label=\"Whisper\")\n",
    "ax.hist(df_utts[\"WER_v\"].dropna(), bins=20, alpha=0.6, label=\"Wav2Vec2\")\n",
    "ax.set_xlabel(\"Utterance WER (%)\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Utterance-level WER distribution\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "# 3) Scatter: duration vs WER (only if you have durations)\n",
    "if \"duration_s\" in df_utts.columns:\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    ax.scatter(df_utts[\"duration_s\"], df_utts[\"WER_w\"], alpha=0.6, label=\"Whisper\")\n",
    "    ax.scatter(df_utts[\"duration_s\"], df_utts[\"WER_v\"], alpha=0.6, label=\"Wav2Vec2\")\n",
    "    ax.set_xlabel(\"Duration (s)\")\n",
    "    ax.set_ylabel(\"WER (%)\")\n",
    "    ax.set_title(\"WER vs duration\")\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "# 4) Speed table (already in df_models, but heres a pretty print)\n",
    "print(\n",
    "    f\"Whisper: wall={wh_wall:.1f}s  RTF={wh_rtf:.3f} | \"\n",
    "    f\"Wav2Vec2: wall={w2v_wall:.1f}s  RTF={w2v_rtf:.3f}\"\n",
    ")\n",
    "\n",
    "# 5) Optional: save CSVs for your thesis appendix\n",
    "df_models.to_csv(\"runs/model_summary.csv\", index=False)\n",
    "df_utts.to_csv(\"runs/utterance_measures.csv\", index=False)\n",
    "print(\"Saved: runs/model_summary.csv and runs/utterance_measures.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4bbbe5-13af-4663-b9a9-197ecb9d99aa",
   "metadata": {},
   "source": [
    "### Fairness results (WER/CER/RTF) for gender and accent groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "206e0257-d2f9-49a0-af78-a2534e7aa700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fairness evaluation utilities ---\n",
    "\n",
    "import os, time, json, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "from jiwer import wer, cer\n",
    "\n",
    "# Pick the w2v function name\n",
    "if 'transcribe_w2v_batch' in globals():\n",
    "    run_w2v = transcribe_w2v_batch\n",
    "elif 'transcribe_w2v2_batch' in globals():\n",
    "    run_w2v = transcribe_w2v2_batch\n",
    "else:\n",
    "    raise NameError(\"Couldn't find transcribe_w2v_batch or transcribe_w2v2_batch in globals().\")\n",
    "\n",
    "# Whisper wrapper is assumed present\n",
    "assert 'transcribe_whisper_paths' in globals(), \"transcribe_whisper_paths(...) not found.\"\n",
    "\n",
    "# Duration cache (faster repeated runs)\n",
    "_DUR_CACHE = {}\n",
    "\n",
    "def get_duration_sec(path):\n",
    "    path = str(path)\n",
    "    if path in _DUR_CACHE:\n",
    "        return _DUR_CACHE[path]\n",
    "    d = float(librosa.get_duration(filename=path))\n",
    "    _DUR_CACHE[path] = d\n",
    "    return d\n",
    "\n",
    "def eval_group(df_sub, whisper_bs=8, w2v_bs=16, label=\"group\"):\n",
    "    \"\"\"Run Whisper and W2V2 on a subset df_sub with 'audio_path' + 'sentence'.\"\"\"\n",
    "    refs  = df_sub['sentence'].tolist()\n",
    "    paths = df_sub['audio_path'].astype(str).tolist()\n",
    "\n",
    "    # total audio length (sec) for RTF\n",
    "    audio_seconds_total = sum(get_duration_sec(p) for p in paths) or 1e-9\n",
    "\n",
    "    # Whisper\n",
    "    t0 = time.time()\n",
    "    hyp_whisper = transcribe_whisper_paths(paths, batch_size=whisper_bs)\n",
    "    wh_wall = time.time() - t0\n",
    "    wh_rtf  = wh_wall / audio_seconds_total\n",
    "    wh_wer  = wer(refs, hyp_whisper) * 100\n",
    "    wh_cer  = cer(refs, hyp_whisper) * 100\n",
    "\n",
    "    # Wav2Vec2\n",
    "    t0 = time.time()\n",
    "    hyp_w2v = run_w2v(paths, batch_size=w2v_bs)\n",
    "    w2v_wall = time.time() - t0\n",
    "    w2v_rtf  = w2v_wall / audio_seconds_total\n",
    "    w2v_wer  = wer(refs, hyp_w2v) * 100\n",
    "    w2v_cer  = cer(refs, hyp_w2v) * 100\n",
    "\n",
    "    return {\n",
    "        \"group\": label,\n",
    "        \"items\": len(refs),\n",
    "        \"audio_s\": audio_seconds_total,\n",
    "        \"Whisper_WER\": wh_wer, \"Whisper_CER\": wh_cer, \"Whisper_wall_s\": wh_wall, \"Whisper_RTF\": wh_rtf,\n",
    "        \"Wav2Vec2_WER\": w2v_wer, \"Wav2Vec2_CER\": w2v_cer, \"Wav2Vec2_wall_s\": w2v_wall, \"Wav2Vec2_RTF\": w2v_rtf,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "687a0815-5fb5-4e3a-872a-8a64b43a867d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender_clean\n",
       "male       754279\n",
       "unknown    605943\n",
       "female     257655\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Cleaning up metadata columns (gender/accent) ---\n",
    "\n",
    "meta = meta.copy()  # using the existing 'meta' from earlier cells\n",
    "\n",
    "# Gender normalization\n",
    "def norm_gender(x):\n",
    "    if not isinstance(x, str): return \"unknown\"\n",
    "    x = x.strip().lower()\n",
    "    if x in (\"male\",\"m\"): return \"male\"\n",
    "    if x in (\"female\",\"f\"): return \"female\"\n",
    "    return \"unknown\"\n",
    "\n",
    "if 'gender' in meta.columns:\n",
    "    meta['gender_clean'] = meta['gender'].map(norm_gender)\n",
    "else:\n",
    "    meta['gender_clean'] = \"unknown\"  # if missing in your TSV\n",
    "\n",
    "# Tiny sanity check\n",
    "meta['gender_clean'].value_counts(dropna=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "160c46d8-e0a3-40c6-b8e8-c07779b6e050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating gender=male on 150 items...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h0/lym4t3rj7pgdbfpw64mhfr700000gn/T/ipykernel_62361/2756402766.py:29: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  d = float(librosa.get_duration(filename=path))\n",
      "python(69090) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69091) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69092) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69093) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69094) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69095) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69096) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69097) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69303) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69305) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69308) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69309) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69310) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69311) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69312) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69313) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69566) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69567) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69568) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69569) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69570) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69571) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69572) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69573) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69759) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69760) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69761) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69762) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69763) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69764) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69765) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69766) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69974) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69975) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69976) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69977) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69978) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69979) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69980) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(69981) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(70176) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(70181) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(70182) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(70183) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(70184) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(70185) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(70186) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(70187) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(70336) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(70338) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(70339) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(70340) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(70341) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(70342) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(70343) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(70344) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(70419) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(70420) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(70421) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(70422) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(70423) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(70424) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(70425) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(70426) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(70748) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(70749) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(70750) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(70751) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(70752) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(70753) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(70754) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(70755) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71035) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71036) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71037) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71038) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71039) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71040) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71041) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71042) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71302) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71303) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71304) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71305) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71306) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71307) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71308) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71309) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71427) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71429) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71430) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71431) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71432) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71433) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71434) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71435) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71547) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71548) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71549) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71550) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71551) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71552) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71553) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71554) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71762) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71763) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71764) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71765) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71766) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71767) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71768) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71769) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71976) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71977) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71978) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71979) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71980) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71981) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71982) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71983) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(72117) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(72118) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(72119) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(72120) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(72121) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(72123) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(72124) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(72125) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(72414) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(72415) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(72416) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(72417) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(72418) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(72419) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(72420) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(72421) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(72723) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(72724) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(72725) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(72726) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(72727) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(72728) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(72729) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(72730) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(72891) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(72892) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(72893) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(72894) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(72895) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(72896) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating gender=female on 150 items...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h0/lym4t3rj7pgdbfpw64mhfr700000gn/T/ipykernel_62361/2756402766.py:29: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  d = float(librosa.get_duration(filename=path))\n",
      "python(72952) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(72953) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(72954) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(72955) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(72956) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(72957) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(72958) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(72959) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73089) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73090) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73091) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73092) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73093) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73094) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73095) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73096) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73244) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73245) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73246) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73247) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73248) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73249) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73250) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73251) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73517) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73518) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73519) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73520) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73521) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73522) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73523) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73524) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73782) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73783) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73784) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73785) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73786) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73787) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73788) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73789) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73993) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73994) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73995) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73996) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73997) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73998) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(73999) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(74000) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(74430) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(74431) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(74432) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(74433) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(74434) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(74435) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(74436) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(74437) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(74636) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(74639) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(74640) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(74641) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(74642) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(74644) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(74645) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(74647) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(74771) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(74772) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(74773) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(74774) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(74775) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(74776) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(74778) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(74779) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(74992) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(74993) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(74994) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(74995) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(74996) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(74997) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(74998) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(74999) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(75375) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(75376) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(75377) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(75378) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(75379) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(75380) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(75381) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(75382) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(75632) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(75634) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(75635) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(75636) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(75637) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(75638) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(75639) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(75640) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(75729) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(75730) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(75731) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(75732) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(75733) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(75734) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(75735) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(75736) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(75904) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(75905) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(75906) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(75907) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(75908) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(75909) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(75910) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(75911) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(76045) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(76046) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(76047) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(76048) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(76049) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(76050) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(76051) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(76052) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(76123) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(76124) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(76125) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(76126) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(76127) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(76128) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(76129) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(76130) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(76202) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(76203) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(76204) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(76205) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(76206) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(76207) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(76208) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(76210) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(76923) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(76926) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(76927) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(76928) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(76929) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(76930) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(76931) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(76932) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(77437) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(77438) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(77439) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(77440) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(77441) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(77442) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>items</th>\n",
       "      <th>audio_s</th>\n",
       "      <th>Whisper_WER</th>\n",
       "      <th>Whisper_CER</th>\n",
       "      <th>Whisper_wall_s</th>\n",
       "      <th>Whisper_RTF</th>\n",
       "      <th>Wav2Vec2_WER</th>\n",
       "      <th>Wav2Vec2_CER</th>\n",
       "      <th>Wav2Vec2_wall_s</th>\n",
       "      <th>Wav2Vec2_RTF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gender:male</td>\n",
       "      <td>150</td>\n",
       "      <td>794.034909</td>\n",
       "      <td>26.04930</td>\n",
       "      <td>5.742485</td>\n",
       "      <td>17191.689504</td>\n",
       "      <td>21.651050</td>\n",
       "      <td>44.570286</td>\n",
       "      <td>11.283087</td>\n",
       "      <td>93.196485</td>\n",
       "      <td>0.117371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender:female</td>\n",
       "      <td>150</td>\n",
       "      <td>799.905167</td>\n",
       "      <td>28.07377</td>\n",
       "      <td>7.401335</td>\n",
       "      <td>73256.965750</td>\n",
       "      <td>91.582063</td>\n",
       "      <td>43.920765</td>\n",
       "      <td>12.390210</td>\n",
       "      <td>84.790465</td>\n",
       "      <td>0.106001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           group  items     audio_s  Whisper_WER  Whisper_CER  Whisper_wall_s  \\\n",
       "0    gender:male    150  794.034909     26.04930     5.742485    17191.689504   \n",
       "1  gender:female    150  799.905167     28.07377     7.401335    73256.965750   \n",
       "\n",
       "   Whisper_RTF  Wav2Vec2_WER  Wav2Vec2_CER  Wav2Vec2_wall_s  Wav2Vec2_RTF  \n",
       "0    21.651050     44.570286     11.283087        93.196485      0.117371  \n",
       "1    91.582063     43.920765     12.390210        84.790465      0.106001  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Evaluate by gender (male/female) ---\n",
    "\n",
    "groups = []\n",
    "for g in [\"male\", \"female\"]:\n",
    "    df_g = meta[meta['gender_clean'] == g]\n",
    "    # Optionally limiting the size\n",
    "    df_g = df_g.sample(min(len(df_g), 150), random_state=42) \n",
    "\n",
    "    if len(df_g) >= 10:\n",
    "        print(f\"Evaluating gender={g} on {len(df_g)} items...\")\n",
    "        groups.append(eval_group(df_g, label=f\"gender:{g}\"))\n",
    "\n",
    "df_gender = pd.DataFrame(groups)\n",
    "df_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4ec6bfb-3c44-4ca6-aaff-a6f3fade886d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Plot WER by gender ---\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10,4), sharey=True)\n",
    "\n",
    "if not df_gender.empty:\n",
    "    ax[0].bar(df_gender['group'], df_gender['Whisper_WER'], color=\"#1f77b4\", alpha=0.85)\n",
    "    ax[0].set_title(\"Whisper WER by gender\")\n",
    "    ax[0].set_ylabel(\"WER (%)\")\n",
    "    ax[0].set_ylim(0, max(df_gender[['Whisper_WER','Wav2Vec2_WER']].max()) * 1.2)\n",
    "\n",
    "    ax[1].bar(df_gender['group'], df_gender['Wav2Vec2_WER'], color=\"#ff7f0e\", alpha=0.85)\n",
    "    ax[1].set_title(\"Wav2Vec2 WER by gender\")\n",
    "\n",
    "    plt.suptitle(\"Fairness: Gender subsets\")\n",
    "    plt.tight_layout()\n",
    "else:\n",
    "    plt.text(0.5,0.5,\"No gender data available\", ha='center')\n",
    "\n",
    "# Save\n",
    "os.makedirs(\"runs\", exist_ok=True)\n",
    "fig.savefig(\"runs/fairness_gender_wer.png\", dpi=150)\n",
    "df_gender.to_csv(\"runs/fairness_gender_summary.csv\", index=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5841b247-e22e-486e-9a02-8a650dcf70af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Accent normalization + pick top-K accents ---\n",
    "\n",
    "def norm_accent(x):\n",
    "    if not isinstance(x, str): return \"unknown\"\n",
    "    x = x.strip().lower()\n",
    "    return x if len(x) else \"unknown\"\n",
    "\n",
    "if 'accent' in meta.columns:\n",
    "    meta['accent_clean'] = meta['accent'].map(norm_accent)\n",
    "else:\n",
    "    meta['accent_clean'] = \"unknown\"\n",
    "\n",
    "# Top-K accents by count (ignore 'unknown' if you wish)\n",
    "K = 5\n",
    "accent_counts = (meta[meta['accent_clean']!=\"unknown\"]\n",
    "                 .groupby('accent_clean').size().sort_values(ascending=False))\n",
    "top_accents = accent_counts.head(K).index.tolist()\n",
    "top_accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e01eace6-4e68-463b-8283-093926ed1d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['client_id', 'path', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accents', 'locale', 'segment', 'audio_path', 'gender_clean', 'accent_clean']\n",
      "                                           client_id  \\\n",
      "0  000abb3006b78ea4c1144e55d9d158f05a9db011016051...   \n",
      "1  0013037a1d45cc33460806cc3f8ecee9d536c45639ba4c...   \n",
      "2  0014c5a3e5715a54855257779b89c2bb498d470b225866...   \n",
      "\n",
      "                           path  \\\n",
      "0  common_voice_en_27710027.mp3   \n",
      "1    common_voice_en_699711.mp3   \n",
      "2  common_voice_en_21953345.mp3   \n",
      "\n",
      "                                            sentence  up_votes  down_votes  \\\n",
      "0  Joe Keaton disapproved of films, and Buster al...         3           1   \n",
      "1                               She'll be all right.         2           1   \n",
      "2                                                six         3           2   \n",
      "\n",
      "   age gender accents locale    segment  \\\n",
      "0  NaN    NaN     NaN     en        NaN   \n",
      "1  NaN    NaN     NaN     en        NaN   \n",
      "2  NaN    NaN     NaN     en  Benchmark   \n",
      "\n",
      "                                          audio_path gender_clean accent_clean  \n",
      "0  /Users/ninadjoshi/asr-comparative-study/ASR-Co...      unknown      unknown  \n",
      "1  /Users/ninadjoshi/asr-comparative-study/ASR-Co...      unknown      unknown  \n",
      "2  /Users/ninadjoshi/asr-comparative-study/ASR-Co...      unknown      unknown  \n"
     ]
    }
   ],
   "source": [
    "print(\"Columns:\", meta.columns.tolist())\n",
    "print(meta.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac38a8d2-4b7b-4983-ab14-c704e0a28f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Using accent column: accents\n",
      "Top accents by count:\n",
      " accent_clean\n",
      "united states english                                            389397\n",
      "england english                                                  134595\n",
      "india and south asia (india, pakistan, sri lanka)                101067\n",
      "canadian english                                                  61132\n",
      "australian english                                                51593\n",
      "german english,non native speaker                                 42001\n",
      "scottish english                                                  15820\n",
      "new zealand english                                               12381\n",
      "irish english                                                      9607\n",
      "southern african (south africa, zimbabwe, namibia)                 8485\n",
      "northern irish                                                     6897\n",
      "filipino                                                           5173\n",
      "hong kong english                                                  4318\n",
      "singaporean english                                                3402\n",
      "liverpool english,lancashire english,england english               2625\n",
      "england english,new zealand english                                2074\n",
      "malaysian english                                                  1810\n",
      "welsh english                                                      1759\n",
      "west indies and bermuda (bahamas, bermuda, jamaica, trinidad)       715\n",
      "southern united states,united states english                        391\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'ref'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mamba/envs/audio/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'ref'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     44\u001b[39m meta[\u001b[33m\"\u001b[39m\u001b[33maccent_group\u001b[39m\u001b[33m\"\u001b[39m] = np.where(\n\u001b[32m     45\u001b[39m     meta[\u001b[33m\"\u001b[39m\u001b[33maccent_clean\u001b[39m\u001b[33m\"\u001b[39m].isin(kept), meta[\u001b[33m\"\u001b[39m\u001b[33maccent_clean\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mother\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     46\u001b[39m )\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# If you prefer to drop \"other\", uncomment:\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# meta = meta[meta[\"accent_group\"] != \"other\"].copy()\u001b[39;00m\n\u001b[32m     50\u001b[39m \n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# ---- Step D: compute WER/CER per accent ----\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m df_accent = \u001b[43mmeta\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maccent_group\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mitems\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhisper_WER\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mwer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mref\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhyp_whisper\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWav2Vec2_WER\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mwer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mref\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhyp_w2v\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhisper_CER\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mref\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhyp_whisper\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWav2Vec2_CER\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mref\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhyp_w2v\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m.reset_index()\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# Sort bars nicely (largest WER first, for example)\u001b[39;00m\n\u001b[32m     63\u001b[39m df_accent = df_accent.sort_values(\u001b[33m\"\u001b[39m\u001b[33mWav2Vec2_WER\u001b[39m\u001b[33m\"\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mamba/envs/audio/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:1825\u001b[39m, in \u001b[36mGroupBy.apply\u001b[39m\u001b[34m(self, func, include_groups, *args, **kwargs)\u001b[39m\n\u001b[32m   1823\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[33m\"\u001b[39m\u001b[33mmode.chained_assignment\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1824\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1825\u001b[39m         result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_python_apply_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selected_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1826\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1827\u001b[39m             \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.obj, Series)\n\u001b[32m   1828\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._selection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1829\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._selected_obj.shape != \u001b[38;5;28mself\u001b[39m._obj_with_exclusions.shape\n\u001b[32m   1830\u001b[39m         ):\n\u001b[32m   1831\u001b[39m             warnings.warn(\n\u001b[32m   1832\u001b[39m                 message=_apply_groupings_depr.format(\n\u001b[32m   1833\u001b[39m                     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mapply\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1836\u001b[39m                 stacklevel=find_stack_level(),\n\u001b[32m   1837\u001b[39m             )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mamba/envs/audio/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:1886\u001b[39m, in \u001b[36mGroupBy._python_apply_general\u001b[39m\u001b[34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[39m\n\u001b[32m   1851\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m   1852\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_python_apply_general\u001b[39m(\n\u001b[32m   1853\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1858\u001b[39m     is_agg: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1859\u001b[39m ) -> NDFrameT:\n\u001b[32m   1860\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1861\u001b[39m \u001b[33;03m    Apply function f in python space\u001b[39;00m\n\u001b[32m   1862\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1884\u001b[39m \u001b[33;03m        data after applying f\u001b[39;00m\n\u001b[32m   1885\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1886\u001b[39m     values, mutated = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_grouper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_groupwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1887\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1888\u001b[39m         not_indexed_same = mutated\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mamba/envs/audio/lib/python3.12/site-packages/pandas/core/groupby/ops.py:919\u001b[39m, in \u001b[36mBaseGrouper.apply_groupwise\u001b[39m\u001b[34m(self, f, data, axis)\u001b[39m\n\u001b[32m    917\u001b[39m \u001b[38;5;66;03m# group might be modified\u001b[39;00m\n\u001b[32m    918\u001b[39m group_axes = group.axes\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m res = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[32m    921\u001b[39m     mutated = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(g)\u001b[39m\n\u001b[32m     44\u001b[39m meta[\u001b[33m\"\u001b[39m\u001b[33maccent_group\u001b[39m\u001b[33m\"\u001b[39m] = np.where(\n\u001b[32m     45\u001b[39m     meta[\u001b[33m\"\u001b[39m\u001b[33maccent_clean\u001b[39m\u001b[33m\"\u001b[39m].isin(kept), meta[\u001b[33m\"\u001b[39m\u001b[33maccent_clean\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mother\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     46\u001b[39m )\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# If you prefer to drop \"other\", uncomment:\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# meta = meta[meta[\"accent_group\"] != \"other\"].copy()\u001b[39;00m\n\u001b[32m     50\u001b[39m \n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# ---- Step D: compute WER/CER per accent ----\u001b[39;00m\n\u001b[32m     52\u001b[39m df_accent = meta.groupby(\u001b[33m\"\u001b[39m\u001b[33maccent_group\u001b[39m\u001b[33m\"\u001b[39m).apply(\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m g: pd.Series({\n\u001b[32m     54\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mitems\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(g),\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWhisper_WER\u001b[39m\u001b[33m\"\u001b[39m: wer(\u001b[43mg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mref\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m.tolist(), g[\u001b[33m\"\u001b[39m\u001b[33mhyp_whisper\u001b[39m\u001b[33m\"\u001b[39m].tolist()) * \u001b[32m100\u001b[39m,\n\u001b[32m     56\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWav2Vec2_WER\u001b[39m\u001b[33m\"\u001b[39m: wer(g[\u001b[33m\"\u001b[39m\u001b[33mref\u001b[39m\u001b[33m\"\u001b[39m].tolist(), g[\u001b[33m\"\u001b[39m\u001b[33mhyp_w2v\u001b[39m\u001b[33m\"\u001b[39m].tolist()) * \u001b[32m100\u001b[39m,\n\u001b[32m     57\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWhisper_CER\u001b[39m\u001b[33m\"\u001b[39m: cer(g[\u001b[33m\"\u001b[39m\u001b[33mref\u001b[39m\u001b[33m\"\u001b[39m].tolist(), g[\u001b[33m\"\u001b[39m\u001b[33mhyp_whisper\u001b[39m\u001b[33m\"\u001b[39m].tolist()) * \u001b[32m100\u001b[39m,\n\u001b[32m     58\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWav2Vec2_CER\u001b[39m\u001b[33m\"\u001b[39m: cer(g[\u001b[33m\"\u001b[39m\u001b[33mref\u001b[39m\u001b[33m\"\u001b[39m].tolist(), g[\u001b[33m\"\u001b[39m\u001b[33mhyp_w2v\u001b[39m\u001b[33m\"\u001b[39m].tolist()) * \u001b[32m100\u001b[39m,\n\u001b[32m     59\u001b[39m     })\n\u001b[32m     60\u001b[39m ).reset_index()\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# Sort bars nicely (largest WER first, for example)\u001b[39;00m\n\u001b[32m     63\u001b[39m df_accent = df_accent.sort_values(\u001b[33m\"\u001b[39m\u001b[33mWav2Vec2_WER\u001b[39m\u001b[33m\"\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mamba/envs/audio/lib/python3.12/site-packages/pandas/core/frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mamba/envs/audio/lib/python3.12/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'ref'"
     ]
    }
   ],
   "source": [
    "from jiwer import wer, cer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# ---- Step A: find an accent column if it exists ----\n",
    "ACCENT_CANDIDATES = [\n",
    "    \"accent\", \"accents\", \"speaker_accent\", \"accent_en\", \"accent_clean\"\n",
    "]\n",
    "accent_col = next((c for c in ACCENT_CANDIDATES if c in meta.columns), None)\n",
    "\n",
    "if accent_col is None:\n",
    "    print(\"  No accent-like column found in this dataset. Available columns:\\n\", meta.columns.tolist())\n",
    "else:\n",
    "    print(f\" Using accent column: {accent_col}\")\n",
    "\n",
    "    # ---- Step B: basic cleaning / normalization ----\n",
    "    meta[\"accent_raw\"] = meta[accent_col].astype(str)\n",
    "    meta[\"accent_clean\"] = (\n",
    "        meta[\"accent_raw\"]\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .replace({\"nan\": \"\", \"none\": \"\"})            # if read as string \"nan\"\n",
    "        .replace(r\"^\\s*$\", np.nan, regex=True)       # empty -> NaN\n",
    "    )\n",
    "\n",
    "\n",
    "    # ---- Step C: keeping accents with enough samples ----\n",
    "    counts = meta[\"accent_clean\"].value_counts(dropna=True)\n",
    "    print(\"Top accents by count:\\n\", counts.head(20))\n",
    "\n",
    "    MIN_SAMPLES = 80   # adjust for your slice size\n",
    "    kept = counts[counts >= MIN_SAMPLES].index\n",
    "\n",
    "    if len(kept) == 0:\n",
    "        print(f\"  No accent has {MIN_SAMPLES} samples. Lower MIN_SAMPLES or use a bigger slice.\")\n",
    "    else:\n",
    "        meta[\"accent_group\"] = np.where(\n",
    "            meta[\"accent_clean\"].isin(kept), meta[\"accent_clean\"], \"other\"\n",
    "        )\n",
    "\n",
    "        # meta = meta[meta[\"accent_group\"] != \"other\"].copy()\n",
    "\n",
    "        # ---- Step D: compute WER/CER per accent ----\n",
    "        df_accent = meta.groupby(\"accent_group\").apply(\n",
    "            lambda g: pd.Series({\n",
    "                \"items\": len(g),\n",
    "                \"Whisper_WER\": wer(g[\"ref\"].tolist(), g[\"hyp_whisper\"].tolist()) * 100,\n",
    "                \"Wav2Vec2_WER\": wer(g[\"ref\"].tolist(), g[\"hyp_w2v\"].tolist()) * 100,\n",
    "                \"Whisper_CER\": cer(g[\"ref\"].tolist(), g[\"hyp_whisper\"].tolist()) * 100,\n",
    "                \"Wav2Vec2_CER\": cer(g[\"ref\"].tolist(), g[\"hyp_w2v\"].tolist()) * 100,\n",
    "            })\n",
    "        ).reset_index()\n",
    "\n",
    "        # Sort bars nicely (largest WER first, for example)\n",
    "        df_accent = df_accent.sort_values(\"Wav2Vec2_WER\", ascending=False)\n",
    "        display(df_accent)\n",
    "\n",
    "        # ---- Step E: plot ----\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n",
    "\n",
    "        df_accent.plot.bar(\n",
    "            x=\"accent_group\", y=\"Whisper_WER\", ax=ax[0],\n",
    "            color=\"#1f77b4\", alpha=0.85\n",
    "        )\n",
    "        ax[0].set_title(\"Whisper WER by accent\")\n",
    "        ax[0].set_ylabel(\"WER (%)\")\n",
    "        ax[0].set_xlabel(\"Accent\")\n",
    "\n",
    "        df_accent.plot.bar(\n",
    "            x=\"accent_group\", y=\"Wav2Vec2_WER\", ax=ax[1],\n",
    "            color=\"#ff7f0e\", alpha=0.85\n",
    "        )\n",
    "        ax[1].set_title(\"Wav2Vec2 WER by accent\")\n",
    "        ax[1].set_xlabel(\"Accent\")\n",
    "\n",
    "        plt.suptitle(\"Fairness: Accent subsets\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        os.makedirs(\"runs\", exist_ok=True)\n",
    "        plt.savefig(\"runs/fairness_accent_wer.png\", dpi=150)\n",
    "        df_accent.to_csv(\"runs/fairness_accent_summary.csv\", index=False)\n",
    "        plt.show()\n",
    "        print(\" Saved: runs/fairness_accent_wer.png and runs/fairness_accent_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83c40a56-c083-472b-9d4f-2a8d7efbd988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# sample must have columns: 'ref', 'hyp_whisper', 'hyp_w2v', and some accent column\n",
    "ACCENT_COL = None\n",
    "for c in ['accent_group', 'accents', 'accent_clean', 'accent']:\n",
    "    if c in sample.columns:\n",
    "        ACCENT_COL = c\n",
    "        break\n",
    "assert ACCENT_COL is not None, \"No accent column found in sample!\"\n",
    "\n",
    "# Clean it up\n",
    "sample['accent_group'] = (\n",
    "    sample[ACCENT_COL]\n",
    "    .fillna('unknown')\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    ")\n",
    "\n",
    "# (optional) Collapse very rare accents to 'other' so bars are stable\n",
    "topN = 20\n",
    "top_accents = (\n",
    "    sample['accent_group'].value_counts()\n",
    "    .head(topN)\n",
    "    .index\n",
    ")\n",
    "sample.loc[~sample['accent_group'].isin(top_accents), 'accent_group'] = 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6b42d34-4a8c-4f30-8f71-7bac86358975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Using accent column: accents\n"
     ]
    }
   ],
   "source": [
    "# this MUST be the same frame you used when you generated hyp_whisper / hyp_w2v\n",
    "df_eval = sample.copy()  # <- not the full meta; use your evaluated subset\n",
    "\n",
    "# normalize reference text into a column used by WER/CER\n",
    "def normalize_text(s: str) -> str:\n",
    "    import re\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"[^a-z' ]\", \" \", s)   # keep letters + spaces + apostrophes\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "df_eval[\"ref\"] = df_eval[\"sentence\"].astype(str).map(normalize_text)\n",
    "\n",
    "# picking accent column present in your data\n",
    "accent_col = \"accents\" if \"accents\" in df_eval.columns else \"accent_clean\"\n",
    "print(f\" Using accent column: {accent_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a97582dc-b28a-4112-a8cc-bdca39cc108e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top accents by count:\n",
      " accents\n",
      "unknown                                                 2348\n",
      "United States English                                   1230\n",
      "England English                                          414\n",
      "India and South Asia (India, Pakistan, Sri Lanka)        301\n",
      "Canadian English                                         176\n",
      "Australian English                                       157\n",
      "German English,Non native speaker                        140\n",
      "Scottish English                                          47\n",
      "Irish English                                             32\n",
      "Southern African (South Africa, Zimbabwe, Namibia)        26\n",
      "Northern Irish                                            23\n",
      "New Zealand English                                       22\n",
      "Filipino                                                  17\n",
      "Hong Kong English                                         10\n",
      "Malaysian English                                          9\n",
      "Liverpool English,Lancashire English,England English       9\n",
      "Singaporean English                                        5\n",
      "Welsh English                                              5\n",
      "England English,New Zealand English                        4\n",
      "New Zealand English,England English                        2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "top_accents = (\n",
    "    df_eval[accent_col].fillna(\"unknown\").value_counts().head(20).index.tolist()\n",
    ")\n",
    "print(\"Top accents by count:\\n\", df_eval[accent_col].fillna(\"unknown\").value_counts().head(20))\n",
    "\n",
    "# creating a grouped label with \"Other\" for less frequent accents\n",
    "df_eval[\"accent_group\"] = np.where(\n",
    "    df_eval[accent_col].isin(top_accents),\n",
    "    df_eval[accent_col],\n",
    "    \"other\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08a3b3d7-4f67-4684-9e49-49d44bbb099a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(sample) = 5000\n",
      "Columns in sample: ['client_id', 'path', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accents', 'locale', 'segment', 'audio_path', 'accent_group']\n"
     ]
    }
   ],
   "source": [
    "print(\"len(sample) =\", len(sample))\n",
    "print(\"Columns in sample:\", list(sample.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "38a38a32-c880-4296-988d-4e9d12ab5625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval items: 64\n"
     ]
    }
   ],
   "source": [
    "EVAL_N = 64  # keep this small while iterating\n",
    "df_eval = sample.head(EVAL_N).reset_index(drop=True)\n",
    "paths_eval = df_eval[\"audio_path\"].tolist()\n",
    "print(\"Eval items:\", len(paths_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6ee3f2b6-742f-422e-92b0-506ceff92563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "os.makedirs(\"cache\", exist_ok=True)\n",
    "WHISP_CACHE = \"cache/whisper_medium.json\"\n",
    "W2V_CACHE   = \"cache/wav2vec2_large.json\"\n",
    "\n",
    "def _load_cache(path):\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "def _save_cache(path, data):\n",
    "    tmp = path + \".tmp\"\n",
    "    with open(tmp, \"w\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False)\n",
    "    os.replace(tmp, path)\n",
    "\n",
    "whisp_cache = _load_cache(WHISP_CACHE)   # {audio_path: text}\n",
    "w2v_cache   = _load_cache(W2V_CACHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "63038372-6c63-4413-b6ac-453f8984f2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch, torchaudio, numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def load_w2v():\n",
    "    model_id = \"facebook/wav2vec2-large-960h-lv60-self\"\n",
    "    proc = Wav2Vec2Processor.from_pretrained(model_id)\n",
    "    mdl  = Wav2Vec2ForCTC.from_pretrained(model_id).to(DEVICE).eval()\n",
    "    return proc, mdl\n",
    "\n",
    "w2v_proc, w2v_model = load_w2v()\n",
    "\n",
    "def _load_wave_16k(path):\n",
    "    wav, sr = torchaudio.load(path)\n",
    "    if sr != 16000:\n",
    "        wav = torchaudio.functional.resample(wav, sr, 16000)\n",
    "    # mono\n",
    "    if wav.shape[0] > 1:\n",
    "        wav = wav.mean(dim=0, keepdim=True)\n",
    "    return wav.squeeze(0)  # [T]\n",
    "\n",
    "def transcribe_w2v_missing(paths, batch_size=16):\n",
    "    new = {}\n",
    "    todo = [p for p in paths if p not in w2v_cache]\n",
    "    if not todo:\n",
    "        return new\n",
    "\n",
    "    print(f\"[W2V] transcribing {len(todo)} missing items...\")\n",
    "    for i in range(0, len(todo), batch_size):\n",
    "        batch = todo[i:i+batch_size]\n",
    "        waves = [ _load_wave_16k(p) for p in batch ]\n",
    "        lens  = [ w.numel() for w in waves ]\n",
    "        padded = pad_sequence(waves, batch_first=True)\n",
    "        inputs = w2v_proc(padded, sampling_rate=16000, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            logits = w2v_model(**inputs).logits\n",
    "            pred_ids = torch.argmax(logits, dim=-1)\n",
    "            texts = w2v_proc.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "\n",
    "        # normalize & store\n",
    "        texts = [t.lower().strip() for t in texts]\n",
    "        for p,t in zip(batch, texts):\n",
    "            w2v_cache[p] = t\n",
    "            new[p] = t\n",
    "\n",
    "        _save_cache(W2V_CACHE, w2v_cache)\n",
    "        print(f\"  [W2V] {i+len(batch)}/{len(todo)}\")\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ab76c87a-b6fa-4a6c-becb-4f64e05853ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(87680) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9a2467c35454cfd87900ed2ee1bbd1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce66616351664f1bbaea6fdd01d71b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/805 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d4aa71b74543efbea10e8d3efe9cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82818955296f46a5a7981cd970d98c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7001cdca92e42ed803c36648773ec65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cbee3b9531b46419f44f179edb401e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fdfe46130d5425a905a124ad35ecc9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82a63d447cd14379a334fa394176e2b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f52b91554a43baa6daae921180e381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26591afde2d14a2e81a8c0aa8307ad88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ff24dcde7fc4996b16e87c173938da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "def load_whisper():\n",
    "    model_id = \"openai/whisper-small.en\"  # MUCH faster than \"medium\" on CPU\n",
    "    proc = WhisperProcessor.from_pretrained(model_id)\n",
    "    mdl  = WhisperForConditionalGeneration.from_pretrained(model_id).to(DEVICE).eval()\n",
    "    return proc, mdl\n",
    "\n",
    "whisp_proc, whisp_model = load_whisper()\n",
    "\n",
    "GEN_KW = dict(\n",
    "    task=\"transcribe\",\n",
    "    language=\"en\",\n",
    "    num_beams=1,\n",
    "    no_repeat_ngram_size=0,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "def transcribe_whisper_missing(paths):\n",
    "    new = {}\n",
    "    todo = [p for p in paths if p not in whisp_cache]\n",
    "    if not todo:\n",
    "        return new\n",
    "\n",
    "    print(f\"[Whisper] transcribing {len(todo)} missing items...\")\n",
    "    for i,p in enumerate(todo, 1):\n",
    "        # load + log-mel\n",
    "        wav, sr = torchaudio.load(p)\n",
    "        if sr != 16000:\n",
    "            wav = torchaudio.functional.resample(wav, sr, 16000)\n",
    "        if wav.shape[0] > 1:\n",
    "            wav = wav.mean(dim=0, keepdim=True)\n",
    "        features = whisp_proc(wav.squeeze(0).numpy(), sampling_rate=16000, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            ids = whisp_model.generate(features[\"input_features\"], **GEN_KW)\n",
    "            text = whisp_proc.batch_decode(ids, skip_special_tokens=True)[0]\n",
    "\n",
    "        text = text.lower().strip()\n",
    "        whisp_cache[p] = text\n",
    "        new[p] = text\n",
    "\n",
    "        if i % 5 == 0 or i == len(todo):\n",
    "            _save_cache(WHISP_CACHE, whisp_cache)\n",
    "        if i % 10 == 0 or i == len(todo):\n",
    "            print(f\"  [Whisper] {i}/{len(todo)}\")\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d206e116-2217-4853-98c2-2e972974f80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# basics used by the helper\n",
    "import numpy as np, torch, librosa\n",
    "\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TARGET_SR  = 16000            # wav2vec2 expects 16 kHz\n",
    "WHISPER_SR = 16000            # Whisper also uses 16 kHz input\n",
    "\n",
    "# need these to define the function again for the transcribe to work\n",
    "w2v_proc  = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\")\n",
    "w2v_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\").to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "df7d2881-26f3-4a97-b614-c5b3b845e2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: [' They are indigenous and peace loving community.']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, librosa, torch\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# 1) Model + processor\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "WHISPER_ID = \"openai/whisper-medium\"  # change if using an .en model\n",
    "WHISPER_SR = 16000\n",
    "\n",
    "wh_proc  = WhisperProcessor.from_pretrained(WHISPER_ID)\n",
    "wh_model = WhisperForConditionalGeneration.from_pretrained(WHISPER_ID).to(DEVICE).eval()\n",
    "\n",
    "# 2) Detect English-only vs multilingual and set flags properly\n",
    "IS_EN_ONLY = WHISPER_ID.endswith(\".en\")\n",
    "if IS_EN_ONLY:\n",
    "    # English-only\n",
    "    GEN_KW      = {}         # nothing\n",
    "    FORCED_IDS  = None\n",
    "    wh_model.generation_config.is_multilingual = False\n",
    "else:\n",
    "    # Multilingual: pass task/language and force decoder ids\n",
    "    GEN_KW      = dict(task=\"transcribe\", language=\"en\")\n",
    "    FORCED_IDS  = wh_proc.get_decoder_prompt_ids(**GEN_KW)\n",
    "    wh_model.generation_config.is_multilingual = True\n",
    "\n",
    "# 3) Load a few WAVs to test (1-D float32, 16 kHz)\n",
    "def load_wavs(paths, sr=WHISPER_SR):\n",
    "    out = []\n",
    "    for p in paths:\n",
    "        y, _ = librosa.load(p, sr=sr, mono=True)\n",
    "        out.append(y.astype(np.float32))\n",
    "    return out\n",
    "\n",
    "# e.g., pick test paths\n",
    "test_paths = [\n",
    "    df_eval[\"audio_path\"].iloc[0],  # or df/sample you already built\n",
    "    # df_eval[\"audio_path\"].iloc[1],\n",
    "]\n",
    "\n",
    "wavs = load_wavs(test_paths, sr=WHISPER_SR)\n",
    "\n",
    "# 4) Run Whisper\n",
    "with torch.no_grad():\n",
    "    inputs = wh_proc(wavs, sampling_rate=WHISPER_SR, return_tensors=\"pt\").to(DEVICE)\n",
    "    feats  = inputs[\"input_features\"]\n",
    "    pred   = wh_model.generate(\n",
    "        feats,\n",
    "        forced_decoder_ids=FORCED_IDS,  # None for .en models\n",
    "        **GEN_KW                           # {} for .en models\n",
    "    )\n",
    "    texts  = wh_proc.batch_decode(pred, skip_special_tokens=True)\n",
    "\n",
    "print(\"OK:\", texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3d9f484e-a988-453d-8194-7079b5f0a196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_whisper_missing(paths,\n",
    "                               batch_size=8,\n",
    "                               device=DEVICE,\n",
    "                               wh_proc=wh_proc,\n",
    "                               wh_model=wh_model,\n",
    "                               sr=16000):\n",
    "    \"\"\"\n",
    "    Transcribe wav paths with Whisper, auto-handling EN-only vs multilingual models.\n",
    "    - Do NOT pass task/lang for EN-only models.\n",
    "    - Set proper flags / forced ids for multilingual models.\n",
    "    \"\"\"\n",
    "\n",
    "    out_texts = []\n",
    "    # --- Deciding generation kwargs based on the *current* model -------------\n",
    "    # Heuristics: .en checkpoints OR config says not multilingual\n",
    "    is_en_only = False\n",
    "    try:\n",
    "        # adding wisper id:\n",
    "        is_en_only = (('WHISPER_ID' in globals() and str(WHISPER_ID).endswith('.en'))\n",
    "                      or not getattr(wh_model.generation_config, \"is_multilingual\", True))\n",
    "    except Exception:\n",
    "        # fall back to config flag only\n",
    "        is_en_only = not getattr(wh_model.generation_config, \"is_multilingual\", True)\n",
    "\n",
    "    if is_en_only:\n",
    "        # English-only: do not pass task/language\n",
    "        GEN_KW = {}\n",
    "        FORCED_IDS = None\n",
    "        wh_model.generation_config.is_multilingual = False\n",
    "    else:\n",
    "        # Multilingual: set task/lang, force the decoder prompt, ensuring flag is true\n",
    "        GEN_KW = dict(task=\"transcribe\", language=\"en\")\n",
    "        FORCED_IDS = wh_proc.get_decoder_prompt_ids(task=\"transcribe\", language=\"en\")\n",
    "        wh_model.generation_config.is_multilingual = True\n",
    "    # -----------------------------------------------------------------------\n",
    "\n",
    "    # batched forward\n",
    "    for i in range(0, len(paths), batch_size):\n",
    "        batch = paths[i:i+batch_size]\n",
    "\n",
    "        # load audio -> 1-D float32 @ 16 kHz\n",
    "        wavs = []\n",
    "        for p in batch:\n",
    "            y, sr_in = librosa.load(p, sr=sr, mono=True)\n",
    "            wavs.append(y.astype(np.float32))\n",
    "        # safety: every item must be 1-D\n",
    "        assert all(np.ndim(w) == 1 for w in wavs), \"each wav must be 1-D\"\n",
    "\n",
    "        # processor makes log-mels, pads & stacks\n",
    "        inputs = wh_proc(\n",
    "            wavs,\n",
    "            sampling_rate=sr,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            feats = inputs[\"input_features\"]                     # [B, 80, T']\n",
    "            pred_ids = wh_model.generate(\n",
    "                feats,\n",
    "                forced_decoder_ids=FORCED_IDS,\n",
    "                **GEN_KW\n",
    "            )\n",
    "            texts = wh_proc.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "\n",
    "        out_texts.extend(t.strip() for t in texts)\n",
    "\n",
    "    return out_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "05debe5b-92c9-4d63-b374-200700cde9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_whisper_paths(paths, whisper_id=\"openai/whisper-medium\", batch_size=8, device=None):\n",
    "    import numpy as np, librosa, torch\n",
    "    from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    sr = 16000\n",
    "\n",
    "    proc  = WhisperProcessor.from_pretrained(whisper_id)\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(whisper_id).to(device).eval()\n",
    "\n",
    "    is_en = whisper_id.endswith(\".en\")\n",
    "    if is_en:\n",
    "        gen_kw = {}\n",
    "        forced_ids = None\n",
    "        model.generation_config.is_multilingual = False\n",
    "    else:\n",
    "        gen_kw = dict(task=\"transcribe\", language=\"en\")\n",
    "        forced_ids = proc.get_decoder_prompt_ids(**gen_kw)\n",
    "        model.generation_config.is_multilingual = True\n",
    "\n",
    "    out = []\n",
    "    for i in range(0, len(paths), batch_size):\n",
    "        chunk = paths[i:i+batch_size]\n",
    "        wavs = []\n",
    "        for p in chunk:\n",
    "            y, _ = librosa.load(p, sr=sr, mono=True)\n",
    "            wavs.append(y.astype(np.float32))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = proc(wavs, sampling_rate=sr, return_tensors=\"pt\").to(device)\n",
    "            feats  = inputs[\"input_features\"]\n",
    "            ids    = model.generate(feats, forced_decoder_ids=forced_ids, **gen_kw)\n",
    "            out.extend(proc.batch_decode(ids, skip_special_tokens=True))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "687109b4-ca99-424c-94a1-5b85e7ba761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect whether we loaded an English-only Whisper or a multilingual one\n",
    "IS_EN_ONLY = WHISPER_ID.endswith(\".en\")\n",
    "\n",
    "if IS_EN_ONLY:\n",
    "    # English-only model\n",
    "    GEN_KW = {}\n",
    "    FORCED_IDS = None\n",
    "    # Make sure generation config reflects English-only\n",
    "    wh_model.generation_config.is_multilingual = False\n",
    "else:\n",
    "    # Multilingual model\n",
    "    GEN_KW = dict(task=\"transcribe\", language=\"en\")\n",
    "    FORCED_IDS = wh_proc.get_decoder_prompt_ids(**GEN_KW)\n",
    "    wh_model.generation_config.is_multilingual = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bc2977fb-4a54-49e0-a600-b66a1d6ffb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    inputs = wh_proc(wavs, sampling_rate=16000, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "    input_features = inputs[\"input_features\"]\n",
    "\n",
    "    pred_ids = wh_model.generate(\n",
    "        input_features,\n",
    "        forced_decoder_ids=FORCED_IDS,\n",
    "        **GEN_KW  # will be {} for .en models, or {task, language} for multilingual\n",
    "    )\n",
    "    texts = wh_proc.batch_decode(pred_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b0e5da9b-0d9e-4e6a-960d-43bc2e17f725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing in w2v_cache: 64\n",
      "Missing in whisp_cache: 64\n",
      "w2v examples: ['/Users/ninadjoshi/asr-comparative-study/ASR-Comparative-Study-Thesis/asr-comparative-study/notebooks/data/cv11_en/cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_21940789.mp3', '/Users/ninadjoshi/asr-comparative-study/ASR-Comparative-Study-Thesis/asr-comparative-study/notebooks/data/cv11_en/cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_23332171.mp3', '/Users/ninadjoshi/asr-comparative-study/ASR-Comparative-Study-Thesis/asr-comparative-study/notebooks/data/cv11_en/cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100999.mp3']\n",
      "whisp examples: ['/Users/ninadjoshi/asr-comparative-study/ASR-Comparative-Study-Thesis/asr-comparative-study/notebooks/data/cv11_en/cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_21940789.mp3', '/Users/ninadjoshi/asr-comparative-study/ASR-Comparative-Study-Thesis/asr-comparative-study/notebooks/data/cv11_en/cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_23332171.mp3', '/Users/ninadjoshi/asr-comparative-study/ASR-Comparative-Study-Thesis/asr-comparative-study/notebooks/data/cv11_en/cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100999.mp3']\n"
     ]
    }
   ],
   "source": [
    "missing_w2v   = [p for p in paths_eval if p not in w2v_cache]\n",
    "missing_whisp = [p for p in paths_eval if p not in whisp_cache]\n",
    "\n",
    "print(\"Missing in w2v_cache:\",   len(missing_w2v))\n",
    "print(\"Missing in whisp_cache:\", len(missing_whisp))\n",
    "\n",
    "# couple examples:\n",
    "print(\"w2v examples:\",   missing_w2v[:3])\n",
    "print(\"whisp examples:\", missing_whisp[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "daf33eac-eb2c-4030-abef-c56575e088ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_w2v_missing(paths, batch_size=16):\n",
    "    todo = [p for p in paths if p not in w2v_cache]\n",
    "    if not todo:\n",
    "        return\n",
    "    out_texts = []\n",
    "    for i in range(0, len(todo), batch_size):\n",
    "        batch = todo[i:i+batch_size]\n",
    "        wavs = []\n",
    "        for p in batch:\n",
    "            y, sr = librosa.load(p, sr=TARGET_SR, mono=True)\n",
    "            wavs.append(y.astype(np.float32))\n",
    "        assert all(np.ndim(w)==1 for w in wavs)  # must be 1-D\n",
    "        inputs = w2v_proc(wavs, sampling_rate=TARGET_SR, return_tensors=\"pt\", padding=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            logits  = w2v_model(**inputs).logits\n",
    "            pred_ids = torch.argmax(logits, dim=-1)\n",
    "            texts = w2v_proc.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "        out_texts.extend([t.lower().strip() for t in texts])\n",
    "    for p, t in zip(todo, out_texts):\n",
    "        w2v_cache[p] = t\n",
    "\n",
    "def transcribe_whisper_missing(paths, batch_size=8):\n",
    "    todo = [p for p in paths if p not in whisp_cache]\n",
    "    if not todo:\n",
    "        return\n",
    "    out_texts = []\n",
    "    for i in range(0, len(todo), batch_size):\n",
    "        batch = todo[i:i+batch_size]\n",
    "        wavs = [librosa.load(p, sr=WHISPER_SR, mono=True)[0] for p in batch]\n",
    "        feats = wh_proc(np.vstack([w[np.newaxis,:] for w in wavs]), sampling_rate=WHISPER_SR, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # IMPORTANT: English-only vs multilingual guard you already added:\n",
    "        if IS_EN_ONLY:\n",
    "            gen_ids = wh_model.generate(feats[\"input_features\"])\n",
    "        else:\n",
    "            gen_ids = wh_model.generate(\n",
    "                feats[\"input_features\"],\n",
    "                **GEN_KW,\n",
    "                forced_decoder_ids=FORCED_IDS\n",
    "            )\n",
    "\n",
    "        texts = wh_proc.batch_decode(gen_ids, skip_special_tokens=True)\n",
    "        out_texts.extend([t.lower().strip() for t in texts])\n",
    "    for p, t in zip(todo, out_texts):\n",
    "        whisp_cache[p] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cf002517-1cfd-42bf-97f3-b48dbb8e0362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 missing w2v;  64 missing whisper\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 66432 and the array at index 1 has size 145536",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# 2) filling only the gaps\u001b[39;00m\n\u001b[32m      7\u001b[39m transcribe_w2v_missing(paths_eval)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mtranscribe_whisper_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# 3) rebuilding for safety\u001b[39;00m\n\u001b[32m     11\u001b[39m hyp_w2v     = [w2v_cache[p]   \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m paths_eval]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mtranscribe_whisper_missing\u001b[39m\u001b[34m(paths, batch_size)\u001b[39m\n\u001b[32m     28\u001b[39m batch = todo[i:i+batch_size]\n\u001b[32m     29\u001b[39m wavs = [librosa.load(p, sr=WHISPER_SR, mono=\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m feats = wh_proc(\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mw\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnewaxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwavs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, sampling_rate=WHISPER_SR, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(device)\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# IMPORTANT: English-only vs multilingual guard you already added:\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m IS_EN_ONLY:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mamba/envs/audio/lib/python3.12/site-packages/numpy/core/shape_base.py:289\u001b[39m, in \u001b[36mvstack\u001b[39m\u001b[34m(tup, dtype, casting)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m    288\u001b[39m     arrs = [arrs]\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 66432 and the array at index 1 has size 145536"
     ]
    }
   ],
   "source": [
    "# 1) finding whats missing\n",
    "missing_w2v   = [p for p in paths_eval if p not in w2v_cache]\n",
    "missing_whisp = [p for p in paths_eval if p not in whisp_cache]\n",
    "print(len(missing_w2v), \"missing w2v; \", len(missing_whisp), \"missing whisper\")\n",
    "\n",
    "# 2) filling only the gaps\n",
    "transcribe_w2v_missing(paths_eval)\n",
    "transcribe_whisper_missing(paths_eval)\n",
    "\n",
    "# 3) rebuilding for safety\n",
    "hyp_w2v     = [w2v_cache[p]   for p in paths_eval]\n",
    "hyp_whisper = [whisp_cache[p] for p in paths_eval]\n",
    "assert len(paths_eval) == len(hyp_whisper) == len(hyp_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "603b8b73-9c42-491e-b589-283a34d0c751",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 66432 and the array at index 1 has size 145536",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m _ = transcribe_w2v_missing(paths_eval)       \u001b[38;5;66;03m# fills w2v_cache incrementally\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m _ = \u001b[43mtranscribe_whisper_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths_eval\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# fills whisp_cache incrementally\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Build lists in the same order as df_eval\u001b[39;00m\n\u001b[32m      5\u001b[39m hyp_w2v     = [ w2v_cache[p]   \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m paths_eval ]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mtranscribe_whisper_missing\u001b[39m\u001b[34m(paths, batch_size)\u001b[39m\n\u001b[32m     28\u001b[39m batch = todo[i:i+batch_size]\n\u001b[32m     29\u001b[39m wavs = [librosa.load(p, sr=WHISPER_SR, mono=\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m feats = wh_proc(\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mw\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnewaxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwavs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, sampling_rate=WHISPER_SR, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(device)\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# IMPORTANT: English-only vs multilingual guard you already added:\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m IS_EN_ONLY:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mamba/envs/audio/lib/python3.12/site-packages/numpy/core/shape_base.py:289\u001b[39m, in \u001b[36mvstack\u001b[39m\u001b[34m(tup, dtype, casting)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m    288\u001b[39m     arrs = [arrs]\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 66432 and the array at index 1 has size 145536"
     ]
    }
   ],
   "source": [
    "_ = transcribe_w2v_missing(paths_eval)       # fills w2v_cache incrementally\n",
    "_ = transcribe_whisper_missing(paths_eval)   # fills whisp_cache incrementally\n",
    "\n",
    "# Build lists in the same order as df_eval\n",
    "hyp_w2v     = [ w2v_cache[p]   for p in paths_eval ]\n",
    "hyp_whisper = [ whisp_cache[p] for p in paths_eval ]\n",
    "\n",
    "print(\"Lengths:\",\n",
    "      len(paths_eval), len(hyp_whisper), len(hyp_w2v))\n",
    "assert len(paths_eval) == len(hyp_whisper) == len(hyp_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e4b267-9069-428b-840e-47f2e0e3ed34",
   "metadata": {},
   "source": [
    "## Noise Robustness Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f386817e-c68a-47d2-a771-cee6114b9ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51865, 768, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=768, out_features=51865, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, WhisperForConditionalGeneration, WhisperProcessor\n",
    "import torch\n",
    "\n",
    "# Load Wav2Vec2 (English pretrained)\n",
    "w2v_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "w2v_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "w2v_model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load Whisper (small English model as example)\n",
    "whisper_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "whisper_processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "whisper_model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8e0b8247-f130-4e5b-b367-190bfddee759",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Whisper\": (whisper_model, whisper_processor),\n",
    "    \"Wav2Vec\": (w2v_model, w2v_processor),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "67b5591a-c32c-4b71-bdef-90e37aa064e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "def add_noise(audio, snr_db):\n",
    "    \"\"\"\n",
    "    Add Gaussian noise to an audio signal at a specified SNR.\n",
    "    :param audio: numpy array of audio samples\n",
    "    :param snr_db: desired SNR in dB (higher = cleaner, lower = noisier)\n",
    "    :return: noisy audio array\n",
    "    \"\"\"\n",
    "    signal_power = np.mean(audio**2)\n",
    "    snr_linear = 10 ** (snr_db / 10.0)\n",
    "    noise_power = signal_power / snr_linear\n",
    "    noise = np.random.normal(0, np.sqrt(noise_power), audio.shape)\n",
    "    return audio + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "898fb5bb-9c63-4c08-9974-fe6a4005689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import wer\n",
    "\n",
    "def evaluate_model_on_noise(model, processor, audio_path, transcript, snr_levels=[20, 10, 0]):\n",
    "    results = {}\n",
    "    audio, sr = librosa.load(audio_path, sr=16000)\n",
    "\n",
    "    # Clean\n",
    "    inputs = processor(audio, sampling_rate=sr, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs.input_values).logits\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    prediction = processor.batch_decode(pred_ids)[0].lower()\n",
    "    results[\"clean\"] = {\"prediction\": prediction, \"wer\": wer(transcript.lower(), prediction)}\n",
    "\n",
    "    # Noisy\n",
    "    for snr in snr_levels:\n",
    "        noisy_audio = add_noise(audio, snr_db=snr)\n",
    "        inputs = processor(noisy_audio, sampling_rate=sr, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(inputs.input_values).logits\n",
    "        pred_ids = torch.argmax(logits, dim=-1)\n",
    "        prediction = processor.batch_decode(pred_ids)[0].lower()\n",
    "        results[f\"{snr}dB\"] = {\"prediction\": prediction, \"wer\": wer(transcript.lower(), prediction)}\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a1c4d68f-dcea-4bde-abeb-79f4c1fbcc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxrwxr-x  0 1000   1000        0 13 Dec  2020 cv-corpus-11.0-2022-09-21/en/\n",
      "drwxrwxr-x  0 1000   1000        0 16 Sep  2022 cv-corpus-11.0-2022-09-21/en/clips/\n",
      "-rw-r--r--  0 1000   1000  3790336 16 Sep  2022 cv-corpus-11.0-2022-09-21/en/dev.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(90652) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  0 1000   1000 62313406 16 Sep  2022 cv-corpus-11.0-2022-09-21/en/invalidated.tsv\n",
      "-rw-r--r--  0 1000   1000 69533545 16 Sep  2022 cv-corpus-11.0-2022-09-21/en/other.tsv\n",
      "-rw-rw-r--  0 1000   1000   648609 17 Sep  2022 cv-corpus-11.0-2022-09-21/en/reported.tsv\n",
      "-rw-r--r--  0 1000   1000  3734423 16 Sep  2022 cv-corpus-11.0-2022-09-21/en/test.tsv\n",
      "-rw-r--r--  0 1000   1000 243129933 16 Sep  2022 cv-corpus-11.0-2022-09-21/en/train.tsv\n",
      "-rw-r--r--  0 1000   1000 396703049 16 Sep  2022 cv-corpus-11.0-2022-09-21/en/validated.tsv\n",
      "-rw-rw-r--  0 1000   1000     18017 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_1.mp3\n",
      "-rw-rw-r--  0 1000   1000     17633 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_10.mp3\n",
      "-rw-rw-r--  0 1000   1000     23969 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100.mp3\n",
      "-rw-rw-r--  0 1000   1000     94113 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_1000.mp3\n",
      "-rw-rw-r--  0 1000   1000     37665 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_10000.mp3\n",
      "-rw-rw-r--  0 1000   1000     15585 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100000.mp3\n",
      "-rw-rw-r--  0 1000   1000     24033 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100001.mp3\n",
      "-rw-rw-r--  0 1000   1000     31329 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100002.mp3\n",
      "-rw-rw-r--  0 1000   1000     17505 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100003.mp3\n",
      "-rw-rw-r--  0 1000   1000     18465 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100004.mp3\n",
      "-rw-rw-r--  0 1000   1000     27105 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100005.mp3\n",
      "-rw-rw-r--  0 1000   1000     22113 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100006.mp3\n",
      "-rw-rw-r--  0 1000   1000     28641 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100007.mp3\n",
      "-rw-rw-r--  0 1000   1000     32097 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100008.mp3\n",
      "-rw-rw-r--  0 1000   1000     30945 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100009.mp3\n",
      "-rw-rw-r--  0 1000   1000     20001 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_10001.mp3\n",
      "-rw-rw-r--  0 1000   1000     20769 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100010.mp3\n",
      "-rw-rw-r--  0 1000   1000     20385 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100011.mp3\n",
      "-rw-rw-r--  0 1000   1000     22305 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100012.mp3\n",
      "-rw-rw-r--  0 1000   1000     18657 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100013.mp3\n",
      "-rw-rw-r--  0 1000   1000     22305 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100014.mp3\n",
      "-rw-rw-r--  0 1000   1000     16737 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100015.mp3\n",
      "-rw-rw-r--  0 1000   1000     20577 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100016.mp3\n",
      "-rw-rw-r--  0 1000   1000     14817 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100017.mp3\n",
      "-rw-rw-r--  0 1000   1000     54177 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100018.mp3\n",
      "-rw-rw-r--  0 1000   1000     48993 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100019.mp3\n",
      "-rw-rw-r--  0 1000   1000     19041 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_10002.mp3\n",
      "-rw-rw-r--  0 1000   1000     50721 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100020.mp3\n",
      "-rw-rw-r--  0 1000   1000     33825 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100021.mp3\n",
      "-rw-rw-r--  0 1000   1000     30945 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100022.mp3\n",
      "-rw-rw-r--  0 1000   1000     56097 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100023.mp3\n",
      "-rw-rw-r--  0 1000   1000     40353 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100024.mp3\n",
      "-rw-rw-r--  0 1000   1000     30561 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100025.mp3\n",
      "-rw-rw-r--  0 1000   1000     49953 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100026.mp3\n",
      "-rw-rw-r--  0 1000   1000     37857 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100027.mp3\n",
      "-rw-rw-r--  0 1000   1000     14625 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100028.mp3\n",
      "-rw-rw-r--  0 1000   1000     34593 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100029.mp3\n",
      "-rw-rw-r--  0 1000   1000     37665 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_10003.mp3\n",
      "-rw-rw-r--  0 1000   1000     39201 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100030.mp3\n",
      "-rw-rw-r--  0 1000   1000     35937 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100031.mp3\n",
      "-rw-rw-r--  0 1000   1000     23073 10 Dec  2020 cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100032.mp3\n"
     ]
    }
   ],
   "source": [
    "!tar -tvf cv-corpus-11.0-2022-09-21-en.tar.gz | head -n 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0c366853-b038-46a4-bd45-e77d1a85d4bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './common_voice_subset/cv-corpus-11.0-2022-09-21/en/test.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      3\u001b[39m base_dir = \u001b[33m\"\u001b[39m\u001b[33m./common_voice_subset/cv-corpus-11.0-2022-09-21/en\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest.tsv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m test_samples = []\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df.head(\u001b[32m5\u001b[39m).iterrows():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mamba/envs/audio/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mamba/envs/audio/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mamba/envs/audio/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mamba/envs/audio/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mamba/envs/audio/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './common_voice_subset/cv-corpus-11.0-2022-09-21/en/test.tsv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd, os\n",
    "\n",
    "base_dir = \"./common_voice_subset/cv-corpus-11.0-2022-09-21/en\"\n",
    "\n",
    "df = pd.read_csv(os.path.join(base_dir, \"test.tsv\"), sep=\"\\t\")\n",
    "\n",
    "test_samples = []\n",
    "for _, row in df.head(5).iterrows():\n",
    "    audio_path = os.path.join(base_dir, \"clips\", row[\"path\"])\n",
    "    transcript = row[\"sentence\"]\n",
    "    test_samples.append((audio_path, transcript))\n",
    "\n",
    "print(test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30b3c1c-02ac-4f6e-a40c-b41f4ffd5e81",
   "metadata": {},
   "source": [
    "## Resource Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dbecabed-06dd-4370-af81-56f6337d5491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, librosa, torch\n",
    "\n",
    "def measure_rtf(model, processor, audio_path, device=\"cpu\"):\n",
    "    # Load audio\n",
    "    audio, sr = librosa.load(audio_path, sr=16000)\n",
    "    duration = len(audio) / sr\n",
    "    \n",
    "    # Prepare input\n",
    "    inputs = processor(audio, sampling_rate=sr, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Time inference\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        _ = model(inputs.input_values).logits\n",
    "    end = time.time()\n",
    "    \n",
    "    return (end - start) / duration  # RTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4d0370f0-807a-48a6-97a1-fff68964ceea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "def get_cpu_memory():\n",
    "    process = psutil.Process()\n",
    "    mem = process.memory_info().rss / 1024**2  # MB\n",
    "    return f\"{mem:.2f} MB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "664fc0f4-825a-4249-9457-b96d127eb391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**2\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**2\n",
    "        return f\"Allocated {allocated:.2f} MB / Reserved {reserved:.2f} MB\"\n",
    "    return \"No GPU available\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f64fe9d9-bedc-465c-8f32-d9ae37c36fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"common_voice_subset/cv-corpus-11.0-2022-09-21/en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a9fd801a-edec-45ec-a481-390d119bbbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: common_voice_subset/cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100001.mp3\n"
     ]
    }
   ],
   "source": [
    "test_audio = \"common_voice_subset/cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100001.mp3\"\n",
    "print(\"Using:\", test_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "82ac87ea-26ec-4613-9124-a776c5078bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa, time, torch\n",
    "\n",
    "def measure_rtf(model, processor, audio_path, device=\"cpu\"):\n",
    "    audio, sr = librosa.load(audio_path, sr=16000)\n",
    "    duration = len(audio) / sr\n",
    "\n",
    "    # Preprocess depending on model type\n",
    "    inputs = processor(audio, sampling_rate=sr, return_tensors=\"pt\")\n",
    "\n",
    "    if \"input_values\" in inputs:   # Wav2Vec2\n",
    "        inputs = inputs.to(device)\n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            _ = model(inputs.input_values).logits\n",
    "        end = time.time()\n",
    "\n",
    "    elif \"input_features\" in inputs:  # Whisper\n",
    "        inputs = inputs.to(device)\n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            _ = model.generate(inputs.input_features)\n",
    "        end = time.time()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model input format\")\n",
    "\n",
    "    return (end - start) / duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "60dd17e4-3343-449e-9b2e-548d90f5a3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
      "Transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     model device       rtf      memory\n",
      "0  Whisper    cpu  1.334538  1290.97 MB\n",
      "1  Whisper    cpu  0.477118  1325.81 MB\n",
      "2  Wav2Vec    cpu  0.316746  1750.34 MB\n",
      "3  Wav2Vec    cpu  0.037239  1756.44 MB\n"
     ]
    }
   ],
   "source": [
    "deploy_df = benchmark_models(models, test_audio)\n",
    "print(deploy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2705f7b6-0a0d-4495-958e-66f5de8caeef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     model device   avg_rtf      memory\n",
      "0  Whisper    cpu  0.789478  1730.75 MB\n",
      "1  Whisper    cpu  0.602564  1677.31 MB\n",
      "2  Wav2Vec    cpu  0.078241  1746.92 MB\n",
      "3  Wav2Vec    cpu  0.038054  1748.03 MB\n"
     ]
    }
   ],
   "source": [
    "def benchmark_avg(models, audio_files):\n",
    "    records = []\n",
    "    for model_name, (model, processor) in models.items():\n",
    "        for device in [\"cpu\", \"cuda\" if torch.cuda.is_available() else \"cpu\"]:\n",
    "            model.to(device)\n",
    "            rtfs = []\n",
    "            for audio in audio_files:\n",
    "                rtf = measure_rtf(model, processor, audio, device=device)\n",
    "                rtfs.append(rtf)\n",
    "            mem = get_gpu_memory() if device==\"cuda\" else get_cpu_memory()\n",
    "            records.append({\n",
    "                \"model\": model_name,\n",
    "                \"device\": device,\n",
    "                \"avg_rtf\": sum(rtfs)/len(rtfs),\n",
    "                \"memory\": mem\n",
    "            })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# Example usage with first 5 clips\n",
    "import glob\n",
    "audio_files = glob.glob(\"common_voice_subset/cv-corpus-11.0-2022-09-21/en/clips/*.mp3\")[:5]\n",
    "deploy_df = benchmark_avg(models, audio_files)\n",
    "print(deploy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce49702-63ac-448b-b7b6-32b5a419b89f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (audio)",
   "language": "python",
   "name": "audio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
